{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Natural Language Processing : coursework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft Research Sentence Completion Challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate number: 250939"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Microsoft Research Sentence Completion Challenge requires a system to be able to predict which is the most likely word (from a set of 5 possibilities) to complete a sentence. In this assignment you are expected to\n",
    "investigate at least 2 extensions or alternative approaches to making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IN this assignmnet I am investigating CBOW and Trigram models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file shows the implementation of Trigram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import os,random,math\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import word_tokenize as tokenize\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd, csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path where the files are\n",
    "mrscc = '/Users/sumitbajare/Documents/Sussex/Sem2/ANLP/lab2resources/sentence-completion'\n",
    "TRAINING_DIR = '/Users/sumitbajare/Documents/Sussex/Sem2/ANLP/lab2resources/sentence-completion/Holmes_Training_Data'\n",
    "questions_file = '/Users/sumitbajare/Documents/Sussex/Sem2/ANLP/lab2resources/sentence-completion/testing_data.csv'\n",
    "answers_file = '/Users/sumitbajare/Documents/Sussex/Sem2/ANLP/lab2resources/sentence-completion/test_answer.csv'\n",
    "\n",
    "results_directory = '/Users/sumitbajare/Documents/Sussex/Sem2/ANLP/lab2resources/sentence-completion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_testing(training_dir,split=0.5):\n",
    "    \n",
    "    '''\n",
    "        split the data between train and test \n",
    "    :param: training_dir, split\n",
    "    :return: trainingfiles, heldoutfiles\n",
    "    '''\n",
    "\n",
    "    filenames=os.listdir(training_dir)\n",
    "    n=len(filenames)\n",
    "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
    "    random.seed(53)  #if you want the same random split every time\n",
    "    random.shuffle(filenames)\n",
    "    index=int(n*split)\n",
    "    trainingfiles=filenames[:index]\n",
    "    heldoutfiles=filenames[index:]\n",
    "    return trainingfiles,heldoutfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class language_model():\n",
    "    \n",
    "    '''\n",
    "        Language model to train a unigram, a bigram, and a trigram model; \n",
    "        Using Kneser-ney smothing, absolute discount\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,known=2,discount=0.75,trainingdir=TRAINING_DIR,files=[]):\n",
    "        \n",
    "        '''\n",
    "            This is the constructor method\n",
    "        :param: known=2,discount,trainingdir, files\n",
    "        :return:none\n",
    "        '''\n",
    "        \n",
    "        self.training_dir = trainingdir\n",
    "        self.files = files\n",
    "        self.discount = discount\n",
    "        self.known = known\n",
    "        self.train()\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        '''\n",
    "            Traning the models\n",
    "        :param:none\n",
    "        :return:none\n",
    "        '''\n",
    "        \n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    "        self.trigram = {}\n",
    "\n",
    "        self.count_token = {}\n",
    "        self.processfiles()\n",
    "        self.make_unknowns()\n",
    "        self.kneser_ney()\n",
    "        self.convert_to_probs()\n",
    "\n",
    "  \n",
    "    def processline(self,line):\n",
    "        \n",
    "        '''\n",
    "            Geting the ngramms for every line of the document\n",
    "        :param: line\n",
    "        :return:none\n",
    "        '''\n",
    "        \n",
    "        tokens = [\"__START\"] + tokenize(line) + [\"__END\"]\n",
    "        previous = \"__END\"\n",
    "        pre_trigram = [\"__END\", \"__END\"]\n",
    "        for token in tokens:\n",
    "          # For getting unigrams\n",
    "            self.unigram[token] = self.unigram.get(token,0) + 1\n",
    "\n",
    "          # Counting the tokens\n",
    "            self.count_token[token] = self.count_token.get(token,0) + 1\n",
    "\n",
    "          # For getting bigrams\n",
    "            current_big = self.bigram.get(previous,{})\n",
    "            current_big[token] = current_big.get(token,0) + 1\n",
    "            self.bigram[previous] = current_big\n",
    "\n",
    "          # For getting trigrams\n",
    "            pre_trigram[1], pre_trigram[0] = pre_trigram[0], pre_trigram[1]\n",
    "            pre_trigram[1] = previous\n",
    "            current_tri = self.trigram.get(tuple(pre_trigram), {})\n",
    "            current_tri[token] = current_tri.get(token, 0) + 1\n",
    "            self.trigram[tuple(pre_trigram)] = current_tri\n",
    "\n",
    "            previous = token\n",
    "\n",
    "    def processfiles(self):\n",
    "        \n",
    "        '''\n",
    "            Processing the file\n",
    "        :param: none\n",
    "        :return:none\n",
    "        '''\n",
    "        \n",
    "        for afile in self.files:\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            self.processline(line)\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
    "\n",
    "          \n",
    "    def convert_to_probs(self):\n",
    "        \n",
    "        '''\n",
    "            Converting ngram counts to probabilities\n",
    "        :param: none\n",
    "        :return:none\n",
    "        '''\n",
    "\n",
    "        self.unigram = {k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
    "        self.bigram = {key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
    "        self.trigram = {key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.trigram.items()}\n",
    "        self.kn = {k:v/sum(self.kn.values()) for (k,v) in self.kn.items()}\n",
    "        self.kn_tri = {k:v/sum(self.kn_tri.values()) for (k,v) in self.kn_tri.items()}\n",
    "\n",
    "    \n",
    "    def get_prob(self,token,context=\"\",methodparams={}):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "            Getting the probability of a token \n",
    "        :param: token,context,methodparams\n",
    "        :return:p\n",
    "        '''\n",
    "        \n",
    "        if methodparams.get(\"method\",\"unigram\") == \"unigram\":\n",
    "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
    "\n",
    "        elif methodparams.get(\"method\",\"bigram\") == \"bigram\": \n",
    "            if methodparams.get(\"smoothing\",\"kneser-ney\") == \"kneser-ney\":\n",
    "                unidist = self.kn\n",
    "            else:\n",
    "                unidist = self.unigram\n",
    "\n",
    "            bigram = self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
    "            big_p = bigram.get(token,bigram.get(\"__UNK\",0))\n",
    "            lmbda = bigram[\"__DISCOUNT\"]\n",
    "            uni_p = unidist.get(token,unidist.get(\"__UNK\",0))\n",
    "            p = big_p + lmbda * uni_p            \n",
    "            return p\n",
    "\n",
    "        elif methodparams.get(\"method\",\"trigram\") == \"trigram\":\n",
    "            if methodparams.get(\"smoothing\",\"kneser-ney\") == \"kneser-ney\":\n",
    "                unidist = self.kn_tri\n",
    "                unidist_bi = self.kn\n",
    "            else:\n",
    "                unidist_bi = self.unigram\n",
    "\n",
    "            if len(context) < 2:\n",
    "                context = [\"__END\", context[0]]\n",
    "        trigram = self.trigram.get(tuple(context[-2:]),self.trigram.get(\"__UNK\",{}))\n",
    "        trig_p = trigram.get(token,trigram.get(\"__UNK\",0))\n",
    "        lmbda_tri = trigram[\"__DISCOUNT\"]\n",
    "\n",
    "        bigram = self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
    "        big_p = bigram.get(token,bigram.get(\"__UNK\",0))\n",
    "        lmbda_bi = bigram[\"__DISCOUNT\"]\n",
    "        uni_p = unidist_bi.get(token,unidist_bi.get(\"__UNK\",0))\n",
    "\n",
    "        p = trig_p + (lmbda_tri * big_p) + (lmbda_bi * uni_p)      \n",
    "        return p\n",
    "  \n",
    "    def compute_prob_line(self,line,methodparams={}):\n",
    "        \n",
    "        '''\n",
    "            Computing the probability of each line of the document\n",
    "        :param: none\n",
    "        :return:acc,len(tokens)\n",
    "        '''\n",
    "        \n",
    "        tokens = [\"__START\"] + tokenize(line) + [\"__END\"]\n",
    "        acc = 0\n",
    "        for i,token in enumerate(tokens[1:]):\n",
    "            acc += math.log(self.get_prob(token,tokens[:i+1],methodparams))\n",
    "        return acc,len(tokens[1:]) # returns probability together with number of tokens\n",
    "\n",
    "    def compute_probability(self,filenames=[],methodparams={}):\n",
    "        \n",
    "        '''\n",
    "            Computing the probability of a corpus contained in filenames\n",
    "        :param:filenames, methodparams\n",
    "        :return:total_p,total_N\n",
    "        '''\n",
    "        \n",
    "        if filenames == []:\n",
    "            filenames = self.files\n",
    "\n",
    "        total_p = 0\n",
    "        total_N = 0\n",
    "        for i,afile in enumerate(filenames):\n",
    "            try:\n",
    "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
    "                    for line in instream:\n",
    "                        line = line.rstrip()\n",
    "                        if len(line) > 0:\n",
    "                            p,N = self.compute_prob_line(line,methodparams=methodparams)\n",
    "                            total_p += p\n",
    "                            total_N += N\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
    "        return total_p,total_N\n",
    "\n",
    "    def compute_perplexity(self,filenames=[],methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}):\n",
    "        \n",
    "        '''\n",
    "            Computing perplexity of the data\n",
    "        :param: filenames,methodparams\n",
    "        :return:pp\n",
    "        '''\n",
    "        \n",
    "        # Lower perplexity means that the model better explains the data\n",
    "\n",
    "        p, N = self.compute_probability(filenames=filenames,methodparams=methodparams)\n",
    "        #print(p,N)\n",
    "        pp = math.exp(-p/N)\n",
    "        return pp  \n",
    "\n",
    "    def make_unknowns(self):\n",
    "        \n",
    "        '''\n",
    "            Making sure the unknown words given some threshold\n",
    "        :param: none\n",
    "        :return:none\n",
    "        '''\n",
    "        \n",
    "        unknown = 0\n",
    "        self.number_unknowns = 0\n",
    "        for (k,v) in list(self.unigram.items()):\n",
    "            if v < self.known:\n",
    "                del self.unigram[k]\n",
    "                self.unigram[\"__UNK\"] = self.unigram.get(\"__UNK\",0) + v\n",
    "                self.number_unknowns += 1\n",
    "\n",
    "        for (k,adict) in list(self.bigram.items()):\n",
    "            for (kk,v) in list(adict.items()):\n",
    "                isknown = self.unigram.get(kk,0)\n",
    "            if isknown == 0 and not kk == \"__DISCOUNT\":\n",
    "                adict[\"__UNK\"] = adict.get(\"__UNK\",0) + v\n",
    "                del adict[kk]\n",
    "            isknown = self.unigram.get(k,0)\n",
    "            if isknown == 0:\n",
    "                del self.bigram[k]\n",
    "                current = self.bigram.get(\"__UNK\",{})\n",
    "                current.update(adict)\n",
    "                self.bigram[\"__UNK\"] = current\n",
    "            else:\n",
    "                self.bigram[k] = adict\n",
    "\n",
    "        for (k,adict) in list(self.trigram.items()):\n",
    "            for (kk,v) in list(adict.items()):\n",
    "                isknown = self.unigram.get(kk,0)\n",
    "                if isknown == 0 and not kk == \"__DISCOUNT\":\n",
    "                    adict[\"__UNK\"] = adict.get(\"__UNK\",0) + v\n",
    "                    del adict[kk]\n",
    "        prev_1, prev_2 = k\n",
    "        isknown_1, isknown_2 = self.unigram.get(prev_1,0), self.unigram.get(prev_2,0)\n",
    "        if isknown_1 == 0 or isknown_2 == 0:\n",
    "            del self.trigram[k]\n",
    "            current = self.trigram.get(\"__UNK\",{})\n",
    "            current.update(adict)\n",
    "            self.trigram[\"__UNK\"] = current\n",
    "        else:\n",
    "            self.trigram[k] = adict\n",
    "\n",
    "              \n",
    "    def kneser_ney(self):\n",
    "        \n",
    "        '''\n",
    "            Apply absolute discount and kneser-Ney smoothing on the models\n",
    "        :param: none\n",
    "        :return:none\n",
    "        '''\n",
    "        # Applying Discount on each bigram \n",
    "        self.bigram = {k:{kk:value-self.discount for (kk,value) in adict.items()} for (k,adict) in self.bigram.items()}\n",
    "\n",
    "        # Applying Discount on each bigram trigram \n",
    "        self.trigram = {k:{kk:value-self.discount for (kk,value) in adict.items()} for (k,adict) in self.trigram.items()}\n",
    "\n",
    "        # To reserve the probability mass store the total amount of the discount\n",
    "        for k in self.bigram.keys():\n",
    "            lamb = len(self.bigram[k])\n",
    "            self.bigram[k][\"__DISCOUNT\"] = lamb * self.discount\n",
    "\n",
    "        for k in self.trigram.keys():\n",
    "            lamb = len(self.trigram[k])\n",
    "            self.trigram[k][\"__DISCOUNT\"] = lamb * self.discount\n",
    "\n",
    "        # kneser-ney unigram prob\n",
    "        self.kn = {}\n",
    "        for (k,adict) in self.bigram.items():\n",
    "            for kk in adict.keys():\n",
    "                self.kn[kk] = self.kn.get(kk,0) + 1\n",
    "\n",
    "        self.kn_tri = {}\n",
    "        for (k,adict) in self.trigram.items():\n",
    "            for kk in adict.keys():\n",
    "                self.kn_tri[kk] = self.kn_tri.get(kk,0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class question:\n",
    "    \n",
    "    def __init__(self,aline):\n",
    "        \n",
    "        '''\n",
    "            This is the constructor method\n",
    "        :param:aline\n",
    "        :return:none\n",
    "        '''\n",
    "        \n",
    "        self.fields=aline\n",
    "  \n",
    "    def get_field(self,field):\n",
    "        \n",
    "        '''\n",
    "            Getting the question field\n",
    "        :param: field\n",
    "        :return:fields\n",
    "        '''\n",
    "        \n",
    "        return self.fields[question.colnames[field]]\n",
    "  \n",
    "    def add_answer(self,fields):\n",
    "        \n",
    "        '''\n",
    "            Getting the answer field\n",
    "        :param: field\n",
    "        :return:none\n",
    "        '''\n",
    "        \n",
    "        self.answer=fields[1]\n",
    "\n",
    "    def get_tokens(self):\n",
    "        \n",
    "        '''\n",
    "            Getting the tokens \n",
    "        :param: none\n",
    "        :return:tokenize\n",
    "        '''\n",
    "        \n",
    "        return [\"__START\"]+tokenize(self.fields[question.colnames[\"question\"]])+[\"__END\"]\n",
    "\n",
    "    def get_left_context(self,window=1,target=\"_____\"):\n",
    "        \n",
    "        '''\n",
    "            Getting the left context\n",
    "        :param: window, target\n",
    "        :return:sent_tokens\n",
    "        '''\n",
    "        \n",
    "        found = -1\n",
    "        sent_tokens = self.get_tokens()\n",
    "        for i,token in enumerate(sent_tokens):\n",
    "            if token == target:\n",
    "                found = i\n",
    "                break  \n",
    "\n",
    "        if found >- 1:\n",
    "            return sent_tokens[i-window:i]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def get_right_context(self,window=1,target=\"_____\"):\n",
    "        \n",
    "        '''\n",
    "            Getting the right context \n",
    "        :param: window,target\n",
    "        :return:sent_tokens\n",
    "        '''\n",
    "        \n",
    "        found = -1\n",
    "        sent_tokens = self.get_tokens()\n",
    "        for i,token in enumerate(sent_tokens):\n",
    "            if token == target:\n",
    "                found = i\n",
    "                break  \n",
    "\n",
    "        if found >- 1:\n",
    "            return sent_tokens[found + 1:found + window + 1]\n",
    "\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def choose(self,lm,method=\"bigram\",smoothing=\"Kneser-ney\", choices=[]):\n",
    "        \n",
    "        '''\n",
    "        \n",
    "            Choose specific ngram; \n",
    "            predicted answer for the sentence \n",
    "        :param: lm,method,smoothing, choices\n",
    "        :return:choice,probs\n",
    "        '''\n",
    "        \n",
    "        if choices == []:\n",
    "            choices = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "\n",
    "        if method == \"bigram\":\n",
    "            rc = self.get_right_context(window=1)\n",
    "            lc = self.get_left_context(window=1)\n",
    "            probs = [lm.get_prob(rc[0],[self.get_field(ch+\")\")],methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing}) * lm.get_prob(self.get_field(ch+\")\"),lc,methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing}) for ch in choices]\n",
    "\n",
    "        elif method == \"trigram\":\n",
    "            rc = self.get_right_context(window=2)\n",
    "            lc = self.get_left_context(window=2)\n",
    "            probs = [lm.get_prob(self.get_field(ch+\")\"), lc, methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing})\n",
    "                    * lm.get_prob(rc[0], [lc[-1]] + [self.get_field(ch+\")\")],methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing})\n",
    "                    * lm.get_prob(rc[1], [self.get_field(ch+\")\")] + [rc[0]],methodparams={\"method\":method.split(\"_\")[0], \"smoothing\":smoothing}) for ch in choices]\n",
    "\n",
    "        else:\n",
    "            context = self.get_left_context(window=1)\n",
    "            probs = [lm.get_prob(self.get_field(ch+\")\"),context,methodparams={\"method\":method.split(\"_\")[0]}) for ch in choices]\n",
    "\n",
    "        maxprob = max(probs)\n",
    "        bestchoices = [ch for ch,prob in zip(choices,probs) if prob == maxprob]\n",
    "\n",
    "        return np.random.choice(bestchoices), probs\n",
    "    \n",
    "    def predict(self,lm,method=\"bigram\", smoothing=\"kneser-ney\"):\n",
    "        \n",
    "        '''\n",
    "            Predict the answer given a language model;\n",
    "            Applying Kneser-Ney smoothing\n",
    "        :param: lm,method, smoothing\n",
    "        :return:choose()\n",
    "        '''\n",
    "        \n",
    "        return self.choose(lm,method=method,smoothing=smoothing,choices=[])\n",
    "\n",
    "    def predict_and_score(self,lm,method=\"bigram\", smoothing=\"kneser-ney\"):\n",
    "        \n",
    "        '''\n",
    "            Checking the prediction is equal to the answer \n",
    "        :param: lm, method, smoothing\n",
    "        :return:1/0\n",
    "        '''\n",
    "        \n",
    "        prediction, probs = self.predict(lm,method=method,smoothing=smoothing)\n",
    "\n",
    "        if prediction == self.answer:\n",
    "            return 1, prediction, probs\n",
    "        else:\n",
    "            return 0, prediction, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scc_reader:\n",
    "    \n",
    "    def __init__(self,qs,ans):\n",
    "        \n",
    "        '''\n",
    "            This is the constructor method\n",
    "        :param: qs, ans\n",
    "        :return:none\n",
    "        '''\n",
    "            \n",
    "        self.qs=qs\n",
    "        self.ans=ans\n",
    "        self.read_files()\n",
    "    \n",
    "    def read_files(self):\n",
    "        \n",
    "        '''\n",
    "            Reading the question and answer file\n",
    "        :param: none\n",
    "        :return:none\n",
    "        '''\n",
    "      \n",
    "        #reading the question file\n",
    "        with open(self.qs) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            qlines=list(csvreader)\n",
    "\n",
    "        question.colnames={item:i for i,item in enumerate(qlines[0])}\n",
    "\n",
    "        #creating a question instance for each line \n",
    "        self.questions=[question(qline) for qline in qlines[1:]]\n",
    "\n",
    "        #reading the answer file\n",
    "        with open(self.ans) as instream:\n",
    "            csvreader=csv.reader(instream)\n",
    "            alines=list(csvreader)\n",
    "\n",
    "        #adding answers to questions     \n",
    "        for q,aline in zip(self.questions,alines[1:]):\n",
    "            q.add_answer(aline)\n",
    "\n",
    "    def get_field(self,field):\n",
    "        \n",
    "        '''\n",
    "            getting the question field \n",
    "        :param: field\n",
    "        :return:get_field\n",
    "        '''\n",
    "        \n",
    "        return [q.get_field(field) for q in self.questions] \n",
    "  \n",
    "    def predict(self,method=\"bigram\"):\n",
    "        \n",
    "        '''\n",
    "        :param: method\n",
    "        :return:predict\n",
    "        '''\n",
    "        \n",
    "        return [q.predict(method=method) for q in self.questions]\n",
    "  \n",
    "    def predict_and_score(self,lm,method=\"bigram\",smoothing=\"kneser-ney\"):\n",
    "        \n",
    "        '''\n",
    "            Computing the accuracy;  \n",
    "            Calculating probability distribution of the options of each question\n",
    "        :param: lm, method, smoothing\n",
    "        :return:sum(scores)/len(scores), predictions, total_probs\n",
    "        '''\n",
    "        \n",
    "        predictions = []\n",
    "        scores = []\n",
    "        total_probs = []\n",
    "        for q in self.questions:\n",
    "            score, pred, probs = q.predict_and_score(lm,method=method, smoothing=smoothing)\n",
    "            scores.append(score)\n",
    "            predictions.append(pred)\n",
    "            total_probs.append(probs)\n",
    "\n",
    "        return sum(scores)/len(scores), predictions, total_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Tuning\n",
    "fls = os.listdir(TRAINING_DIR)\n",
    "number_files = np.linspace(10, 100, 3).astype(int)\n",
    "known = [2, 3, 4]\n",
    "discount = 0.75\n",
    "smoothing = 'kneser-ney'\n",
    "MAX_FILES = 2\n",
    "results = []\n",
    "predictions = []\n",
    "total_probs = []\n",
    "iter = 0\n",
    "for n in number_files:\n",
    "    for k in known:\n",
    "        print('Processing {} documents...'.format(n))\n",
    "        mylm = language_model(known=k, discount=discount, files=fls[:n])\n",
    "        SCC = scc_reader(questions_file, answers_file)\n",
    "\n",
    "        unigram_accuracy, unigram_predictions, unigram_probs = SCC.predict_and_score(mylm,method=\"unigram\",smoothing=smoothing)\n",
    "        bigram_accuracy, bigram_predictions, bigram_probs = SCC.predict_and_score(mylm,method=\"bigram\",smoothing=smoothing)\n",
    "        trigram_accuracy, trigram_predictions, trigram_probs = SCC.predict_and_score(mylm,method=\"trigram\",smoothing=smoothing)\n",
    "\n",
    "        results.append((n, k, unigram_accuracy, bigram_accuracy, trigram_accuracy))\n",
    "        predictions.append((unigram_predictions, bigram_predictions, trigram_predictions))\n",
    "        total_probs.append((unigram_probs, bigram_probs, trigram_probs))\n",
    "\n",
    "        iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['number of files', 'OOV threshold', 'unigram accuracy', 'bigram accuracy', 'trigram accuracy']\n",
    "values = results\n",
    "df = pd.DataFrame(values, columns=columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
